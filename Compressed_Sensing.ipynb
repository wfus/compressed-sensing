{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Compressed Sensing",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wfus/compressed-sensing/blob/master/Compressed_Sensing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "qjcIfKQDroA7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Compressed Sensing Project\n",
        "\n",
        "We will be implementing compressed sensing. We will be using CIFAR-100 as a good dataset for images. The dataset can be downloaded [here.](https://www.cs.toronto.edu/~kriz/cifar.html)"
      ]
    },
    {
      "metadata": {
        "id": "-rG768oAI2qA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data Loading\n",
        "\n",
        "Here is some code that loads in the images in standard numpy format. The format given in CIFAR-100 is with the channel, most likely $C \\times W \\times H$, while we want it $H \\times W \\times C$ to display it using numpy and matplotlib, and for when we feed it into our neural network."
      ]
    },
    {
      "metadata": {
        "id": "gDTc14DeYJEr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy\n",
        "import sys\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IKU7aTf2YQ0V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "wget -nc https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
        "tar xvf cifar-100-python.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RSpAWVlmYoh2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def unpickle(file):\n",
        "    if sys.version_info >= (3,0):\n",
        "        import pickle\n",
        "        with open(file, 'rb') as fo:\n",
        "            d = pickle.load(fo, encoding='bytes')\n",
        "        return d\n",
        "    else:\n",
        "        import cPickle\n",
        "        with open(file, 'rb') as fo:\n",
        "            d = cPickle.load(fo)\n",
        "        return d\n",
        "\n",
        "def cifar_to_image(data):\n",
        "    test_image = data.reshape(3, 32, 32)\n",
        "    test_image = test_image.swapaxes(0, 2)\n",
        "    test_image = test_image.swapaxes(0, 1)\n",
        "    return test_image\n",
        "\n",
        "train_data = unpickle('cifar-100-python/train')\n",
        "test_data = unpickle('cifar-100-python/test')\n",
        "images = train_data[b'data']\n",
        "labels = train_data[b'fine_labels']\n",
        "test_images = test_data[b'data']\n",
        "test_labels = test_data[b'fine_labels']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GhsJnJJ5bzFq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print('Number of images in training set: %d' % len(images))\n",
        "print('Number of images in testing set: %d' % len(test_images))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ve-INyKKdiFv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plot_random_image(test1):\n",
        "    plt.grid(False)\n",
        "    plt.imshow(test1)\n",
        "    plt.show()\n",
        "    \n",
        "def get_random_image():\n",
        "    rand_index = np.random.choice(range(len(images)))\n",
        "    print('Chose index %d from training set' % rand_index)\n",
        "    print('Label: %d' % labels[rand_index])\n",
        "    test1 = cifar_to_image(images[rand_index])\n",
        "    return test1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tQL7ZifQdQdw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test = get_random_image()\n",
        "plot_random_image(test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2WaQ9JN_ryUA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Turning the Image Sparse\n",
        "\n",
        "Compressive sensing works by converting our image into a domain where it is sparse. This is very similar to how we transform into frequency space and quantize in JPEG and other popular image encoding methods. This will apply the DCT transform and some quantization to make our values `uint8`. This is the way most images are encoded before sending it through a network channel.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "WLrgjX5sP2e-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### DCT\n",
        "\n",
        "The DCT can be done in `scipy`, [here](http://bugra.github.io/work/notes/2014-07-12/discre-fourier-cosine-transform-dft-dct-image-compression/) is a nice link as a tutorial. That link applies to the DCT on the full image, but we want to experiment using the DCT on $8 \\times 8$ blocks just like JPEG first. Some code for that is available [here](https://inst.eecs.berkeley.edu/~ee123/sp16/Sections/JPEG_DCT_Demo.html) as a nice Jupyter notebook from Berkeley."
      ]
    },
    {
      "metadata": {
        "id": "eZLf-nrtP4U7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Full Image DCT\n",
        "\n",
        "We'll first try the DCT matrix on the full image and see what happens. We can also see "
      ]
    },
    {
      "metadata": {
        "id": "yVB5ugPnrVEH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import scipy\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oJufMqpRrxjr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from scipy import fftpack as fft\n",
        "\n",
        "def dct_full_2D(image):\n",
        "    return fft.dct(fft.dct(image.T, norm='ortho').T, norm='ortho')\n",
        "\n",
        "def idct_full_2D(coefficients):\n",
        "    return fft.idct(fft.idct(coefficients.T, norm='ortho').T, norm='ortho')\n",
        "\n",
        "def image_to_dct(image):\n",
        "    return np.array(dct_full_2D(image), dtype=np.int32)\n",
        "\n",
        "def dct_to_image(dctmatrix):\n",
        "    return idct_full_2D(dctmatrix).clip(0, 255).astype(np.uint8)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q6csEasCrqX_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dct_test = get_random_image()\n",
        "plot_random_image(dct_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Iz00_yq3TEoW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dct_test_converted = dct_full_2D(dct_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KtbXo-_TZPXY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "N_COLOR_CHANNELS = 3\n",
        "for chan in range(N_COLOR_CHANNELS):\n",
        "    plt.subplot(1, 3, chan + 1)\n",
        "    plt.imshow(dct_test_converted[:, :, chan])\n",
        "    \n",
        "dct_test_int = np.array(dct_test_converted, dtype=np.int32)\n",
        "print('Sparsity of DCT: ', np.sum(dct_test_int == 0) / float(dct_test_int.size))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0vhk9DigZ7RD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We see that this is indeed a pretty sparse representation, once we take the floor of everything to make the coefficients integers. Let's turn the image back once we have done this step to see our recovered image (rounding the DCT coefficients lose some information)"
      ]
    },
    {
      "metadata": {
        "id": "zhaJW2LFZ6cG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plot_random_image(dct_to_image(dct_test_int))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OkRWPaEBsFcy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Retrieval with Data-driven Approaches\n",
        "\n",
        "Let's first try the naive approach. The naive approach we don't expect will work well at all.\n",
        "\n",
        "In the CIFAR case, note that the image values have the support $\\mathbb{R}^{32 \\times 32 \\times 3}$. However, we are assuming that the domain of natural images is actually much much smaller. Through training with the data, we should be able to try to train a blackbox to retrieve images from undersampled measurements."
      ]
    },
    {
      "metadata": {
        "id": "zPcMaJqYcIYZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Making the Sensing Matrix\n",
        "\n",
        "First, let's flatten our DCT coefficients, so our original signal in a sparse space is $\\mathbf{x} \\in \\mathbb{R}^{3072}$. Then, the setup of our problem will be mapping signals from $\\mathbb{R}^{N} \\to \\mathbb{R}^{M}$. For now, we will assume that the sensing matrix is distributed randomly.\n",
        "\n",
        "$$N = 3072$$\n",
        "$$M = \\mathsf{[500, 1000, 1500, 2000]}$$\n",
        "$$x \\in \\mathbb{R}^N$$\n",
        "$$A \\in \\mathbb{R}^{N \\times M}$$\n",
        "$$A_{i, j} \\sim \\mathcal{N}(0, 1)$$\n",
        "$$y = Ax$$\n",
        "\n",
        "We will implement our sensing matrix as a fully-connected layer in our neural network. We will initialize it with a Gaussian, which should give us what we want."
      ]
    },
    {
      "metadata": {
        "id": "HAQQq9QKsgNV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.distributions as ds\n",
        "from torch import nn\n",
        "from torch.utils import data as utils\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gsBBLSTnfs2N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "N = 3072\n",
        "M = 2000\n",
        "test_x = ds.Normal(torch.zeros(N), torch.ones(N)).sample()\n",
        "A = ds.Normal(torch.zeros((M, N)), torch.ones((M, N))).sample()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AIWntl7N84rk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_y = torch.matmul(A, test_x)\n",
        "test_y.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8IhLJmiD8_to",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Preprocessing our Data\n",
        "\n",
        "Our FCN should be mapping the $y$ back to the sparse $x$. Therefore, we will have to get our images, sparsify them, and those will be $x$. The $y$ we feed in to the neural network will be $Ax$. It's kind of confusing because the variables are swapped, but:\n",
        "\n",
        "```\n",
        "train_X: undersampled measurements y = Ax\n",
        "train_y: original sparse flattened DCT of image, x\n",
        "```\n",
        "\n",
        "We will have to preprocess our CIFAR images first."
      ]
    },
    {
      "metadata": {
        "id": "KHFqwIPAKViG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print('Using device: %s' % str(device))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MR5kOq3x8UKs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def cifar_to_sparse(image):\n",
        "    image = cifar_to_image(image)\n",
        "    image = image_to_dct(image)\n",
        "    return image.flatten()\n",
        "\n",
        "sparse_X = torch.tensor([cifar_to_sparse(im) for im in images]).float()\n",
        "test_sparse_X = torch.tensor([cifar_to_sparse(im) for im in test_images]).float()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Hjqa4zBl8UM-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_X = torch.matmul(sparse_X, A.t()).to(device)\n",
        "train_y = sparse_X.to(device)\n",
        "\n",
        "test_X = torch.matmul(test_sparse_X, A.t()).to(device)\n",
        "test_y = test_sparse_X.to(device)\n",
        "\n",
        "train_data = utils.TensorDataset(train_X, train_y)\n",
        "dataloader = utils.DataLoader(train_data, batch_size=64, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q9cNUdNh8URO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class JohnNet(nn.Module):\n",
        "    \"\"\"A NN that definitely won't work, lmao, Test our dataloader.\"\"\"\n",
        "    def __init__(self, N, M, A):\n",
        "        super(JohnNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(M, 100)\n",
        "        self.fc2 = nn.Linear(100, N)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return self.fc2(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "snQdSSnm8UTX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "net = JohnNet(N, M, A).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YsQfXLsm8UVl",
        "colab_type": "code",
        "outputId": "c92c6173-d999-40a4-abbb-820f0270a7e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "epochs = 1000\n",
        "epoch_queue = tqdm.tqdm(range(epochs))\n",
        "for epoch in epoch_queue:\n",
        "    for index, batch in enumerate(dataloader):\n",
        "        X, y = batch\n",
        "        optimizer = torch.optim.Adam(net.parameters())\n",
        "        criterion = nn.MSELoss()\n",
        "    \n",
        "        optimizer.zero_grad()\n",
        "        y_hat = net(X)\n",
        "        loss = criterion(y_hat, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    y_hat = net(test_X)\n",
        "    test_loss = criterion(y_hat, test_y)\n",
        "    epoch_queue.set_description(\"Test Loss: %d\" % test_loss)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 5792: 100%|██████████| 1000/1000 [16:00<00:00,  1.05it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "XsTKyNwyLZcS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now let's look at our predicted images to see if they even look reasonable at all. It probably won't TBQH."
      ]
    },
    {
      "metadata": {
        "id": "7TWOR01_LWeP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def plot_naive_training():\n",
        "    rand_index = random.randint(0, len(train_X) - 1)\n",
        "    test_x_image = train_X[rand_index]\n",
        "    test_y_image = train_y[rand_index]\n",
        "    pred_sparse = net(test_x_image)\n",
        "    pred_image = dct_to_image(pred_sparse.reshape(32, 32, 3).cpu().detach().numpy())\n",
        "    real_image = dct_to_image(test_y_image.reshape(32, 32, 3).cpu().detach().numpy())\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(pred_image)\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(real_image)\n",
        "    plt.show()\n",
        "    \n",
        "def plot_naive_testing():\n",
        "    rand_index = random.randint(0, len(test_X) - 1)\n",
        "    test_x_image = test_X[rand_index]\n",
        "    test_y_image = test_y[rand_index]\n",
        "    pred_sparse = net(test_x_image)\n",
        "    pred_image = dct_to_image(pred_sparse.reshape(32, 32, 3).cpu().detach().numpy())\n",
        "    real_image = dct_to_image(test_y_image.reshape(32, 32, 3).cpu().detach().numpy())\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(pred_image)\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(real_image)\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rvvy2hJQ8UXw",
        "colab_type": "code",
        "outputId": "1421b1c9-8fa8-4aaf-94cc-46c4ad51bbed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "plot_naive_testing()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "00fj3v_8siVG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sensing_fc = nn.Linear(N, M, bias=False)  # If we do decide to actually optimize for the sensing layer."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K8U372h6gFMA",
        "colab_type": "code",
        "outputId": "d585c7e3-4d48-4fe7-d177-42a99f149c4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "test_x"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-1.5866,  0.2789, -0.9628,  ..., -0.1109, -0.9543,  0.8594])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "metadata": {
        "id": "HbBTBfYatXEi",
        "colab_type": "code",
        "outputId": "ec77002a-81f6-48e6-95a3-275b7891c4ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "cell_type": "code",
      "source": [
        "nn.init.normal_(sensing_fc.weight, 0, 1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[ 0.0805, -0.1097,  0.7224,  ...,  2.1265,  0.5065,  0.0935],\n",
              "        [-0.0101, -0.7676,  0.0477,  ..., -0.3958,  0.2011,  0.2303],\n",
              "        [ 0.2383, -1.0049,  0.2266,  ...,  0.5397,  1.0106,  0.6275],\n",
              "        ...,\n",
              "        [ 0.1016, -0.7452,  0.9516,  ..., -0.2994,  1.3512,  0.5771],\n",
              "        [ 0.4895,  2.0024, -0.4702,  ...,  1.0088,  0.6933, -0.1347],\n",
              "        [ 0.7317, -0.1853, -1.8950,  ..., -1.2375,  0.3239,  0.0671]],\n",
              "       requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "metadata": {
        "id": "l5uQrR_WgB8r",
        "colab_type": "code",
        "outputId": "20e9729f-5ef4-4d2f-8b07-8ab1bf1f9d07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        }
      },
      "cell_type": "code",
      "source": [
        "image\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-95-28935580a9bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'image' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "AZN70f6z62SN",
        "colab_type": "code",
        "outputId": "564d68be-1da7-40b4-fb80-d52f5e13b900",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        }
      },
      "cell_type": "code",
      "source": [
        "image"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-241-28935580a9bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'image' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "X6vaGsqdTnSM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Another approach - using the Adjoint\n",
        "\n",
        "What if we feed into our neural network the adjoint without doing the DCT first? Again, we'll essentially be using a linear layer at the beginning and performing retrieval. This is very similar to the paper [Mousavi and baraniuk - Learning to invert: Signal Recovery via Deep Convolutional Networks.](https://arxiv.org/abs/1701.03891)\n",
        "\n",
        "Now, we're going to undersample in a weirder way. We will first have images that are $32 \\times 32 \\times 3$. We are then going to flatten, downsample to dimension $2700$ to get our resultant $y$. Then, we will calculate $\\tilde{x} = A^\\top y$ and feed $\\tilde{x}$ into our FCN. Lets do it!"
      ]
    },
    {
      "metadata": {
        "id": "crfK7i_sTZcN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print('Using device: %s' % str(device))\n",
        "\n",
        "image_X = torch.tensor([cifar_to_image(im) for im in images]).float().to(device)\n",
        "test_image_X = torch.tensor([cifar_to_image(im) for im in test_images]).float().to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yLd2w33QWg6q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(image_X.shape)\n",
        "print(test_image_X.shape)\n",
        "\n",
        "image_X.view(image_X.shape[0], -1).shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zSboupzHUQrR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# For training purposes, we're essentially multiplying by A^T A\n",
        "ATA = A.t().mm(A).to(device)\n",
        "\n",
        "# Flatten, apply our sensing matrix, and multiply by adjoint.\n",
        "train_X = image_X.view(image_X.shape[0], -1).mm(ATA).view(image_X.shape)\n",
        "test_X = test_image_X.view(test_image_X.shape[0], -1).mm(ATA).view(test_image_X.shape)\n",
        "\n",
        "train_y = image_X\n",
        "test_y = test_image_X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ABE7FBG4XT2c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plot_train_test(index):\n",
        "    train_signal = train_X[index].cpu().numpy().astype(np.uint8)\n",
        "    test_signal = train_y[index].cpu().numpy().astype(np.uint8)\n",
        "    \n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(train_signal)\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(test_signal)\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2YQue0-9Yy_W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plot_train_test(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "waoz9vs3XzTa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plot_train_test(2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mQhfX0wDYcs3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}